{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12da72c0",
   "metadata": {},
   "source": [
    "# Week 14 Rubric Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c81f8",
   "metadata": {},
   "source": [
    "### What are 3 advantages of deploying using Model Serving methods Vs. deploying on GitHub Pages or HuggingFace for free?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3598f0",
   "metadata": {},
   "source": [
    "- **Scalability and performance**: Model serving methods can scale to handle high traffic loads and large datasets, allowing for faster and more efficient processing of requests. These platforms are designed to handle a wide range of workloads, including machine learning models, and provide advanced features like auto-scaling, load balancing, and distributed computing, which can improve the performance and reliability of the deployed models.\n",
    "- **Customization and control**: Model serving methods offer greater customization and control over the deployment environment, allowing for fine-tuning of the system to meet specific requirements. This includes the ability to select the type and size of the infrastructure, as well as the operating system, runtime, and dependencies. This can help optimize the deployment for performance, security, and cost.\n",
    "- **Integration and collaboration**: Model serving methods offer integration with other cloud services and tools, such as data storage, monitoring, and analytics, making it easier to build end-to-end machine learning workflows. These platforms also provide collaboration features, such as role-based access control, versioning, and collaboration tools, which can help teams work together more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a30531",
   "metadata": {},
   "source": [
    "### What is ML model deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f19ed",
   "metadata": {},
   "source": [
    "The key steps for deploying a machine learning app are:\n",
    "- **Saving the trained model**: Once a model has been trained, it needs to be saved in a format that can be loaded into a production environment (e.g. pickle). \n",
    "- **Preprocessing and postprocessing**: Before the model can make predictions on new data, the input data needs to be preprocessed to match the format used during training. Similarly, the output of the model may need to be postprocessed to make it usable by downstream applications.\n",
    "- **Creating an API**: A machine learning model is typically deployed as a web service or API that can be accessed by other applications. The API provides a way for other applications to send input data to the model, and receive predictions or insights in response.\n",
    "- **Containerizing the API**: To make the deployment scalable and reliable, the API is typically containerized using containerization platforms such as Docker. Containerization provides a way to package the application, its dependencies, and the model into a single, portable unit that can be deployed and run consistently across different environments.\n",
    "- **Deploying the API to production**: Once the API is containerized, it can be deployed to a production environment, such as a cloud-based platform or on-premise server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad129d",
   "metadata": {},
   "source": [
    "### What is Causal Inference and how Does It Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59335501",
   "metadata": {},
   "source": [
    "Causal inference refers to the process of using statistical and machine learning techniques to draw conclusions about the cause-and-effect relationships between variables in a system. It involves identifying causal relationships between variables and using that information to predict the outcome of interventions or changes in the system.\n",
    "\n",
    "- Defining the problem: The first step in causal inference is to define the problem and the relevant variables in the system. This may involve identifying potential causal variables, potential confounding variables, and the outcome variable of interest.\n",
    "- Collecting and preparing data: The next step is to collect and prepare the data for analysis. This may involve cleaning and preprocessing the data, identifying missing data, and selecting relevant variables for analysis.\n",
    "- Identifying causal relationships: Once the data is prepared, the next step is to identify causal relationships between variables. This may involve using statistical methods such as regression analysis or machine learning techniques to identify relationships between variables and to control for confounding variables.\n",
    "- Validating causal relationships: After identifying potential causal relationships, the next step is to validate them. This may involve using methods such as cross-validation or testing for statistical significance to determine whether the relationship is robust and generalizable.\n",
    "- Using causal relationships to make predictions: Finally, once causal relationships have been validated, they can be used to make predictions about the outcome of interventions or changes in the system. This can help researchers and decision-makers to design effective interventions and make informed decisions based on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac9b34",
   "metadata": {},
   "source": [
    "### What is serverless deployment and how its compared with deployment on server?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e7665",
   "metadata": {},
   "source": [
    "Serverless deployment is a type of cloud computing architecture that allows developers to deploy and run their applications without having to manage the underlying server infrastructure. In a serverless architecture, the cloud provider takes care of all the infrastructure management, including scaling, patching, and updating the servers, allowing developers to focus on writing and deploying their code.\n",
    "\n",
    "In a traditional server-based deployment, the developer is responsible for managing the server infrastructure, including configuring the server, installing and maintaining the necessary software and libraries, and scaling the servers to handle varying workloads. This can be a time-consuming and complex process, and requires significant expertise in system administration and DevOps.\n",
    "\n",
    "By contrast, serverless deployment abstracts away the underlying server infrastructure, allowing developers to focus solely on writing and deploying their code. The cloud provider automatically provisions and scales the required infrastructure to handle incoming requests, and charges the developer based on usage, rather than a fixed monthly cost.\n",
    "\n",
    "Serverless deployment also offers several benefits compared to server-based deployment, including:\n",
    "\n",
    "- Scalability: Serverless deployment allows applications to scale up or down automatically based on demand, without the need for manual intervention or complex scaling rules. This enables applications to handle sudden spikes in traffic and reduces the risk of downtime or performance issues.\n",
    "- Reduced cost: Serverless deployment typically incurs lower costs than traditional server-based deployment, as developers only pay for the resources used by their application, rather than having to pay for an entire server infrastructure.\n",
    "- Increased agility: Serverless deployment enables developers to deploy their code faster and more frequently, as they do not have to worry about managing the underlying infrastructure. This allows teams to be more agile and responsive to changing business requirements.\n",
    "\n",
    "However, serverless deployment also has some limitations, including the need to conform to the constraints of the serverless architecture, and the potential for vendor lock-in. Developers must also be aware of the potential for cold starts, which can cause a slight delay in response time when a new instance of the application is spun up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
