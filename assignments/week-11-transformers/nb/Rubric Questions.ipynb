{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362bf6f8",
   "metadata": {},
   "source": [
    "## Rubric Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49341d",
   "metadata": {},
   "source": [
    "#### In the transformer architecture in the paper \"Attention Is All You Need\", how does Multi-head Attention work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679a6f2",
   "metadata": {},
   "source": [
    "Multi-head attention is a mechanism used in the transformer architecture for natural language processing tasks. The transformer architecture uses attention mechanisms to allow the model to weigh the importance of different words in a sentence or sequence when making predictions.\n",
    "\n",
    "In multi-head attention, the model uses multiple attention heads, each with their own set of parameters, to attend to different parts of the input sequence. By using multiple heads, the model can attend to different parts of the input sequence and capture different types of information. This can help the model to better understand the relationships between words in a sentence, leading to improved performance on natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7de43",
   "metadata": {},
   "source": [
    "#### What is the main idea behind Positional Encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a7643b",
   "metadata": {},
   "source": [
    "The main idea behind Positional Encoding is to provide additional information to the neural network model about the position of a word in a sentence or a sequence. In natural language processing tasks, the position of a word can convey important information about its meaning and context, but this information is not always captured by the word's embedding alone. Positional Encoding is a way of adding this information back into the model, allowing it to better understand the relationships between words in a sentence and make more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f60fe1",
   "metadata": {},
   "source": [
    "#### What is EarlyStopping and why do we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8283e23",
   "metadata": {},
   "source": [
    "Early stopping is a technique used in machine learning to prevent overfitting of a model by stopping the training process before the model becomes too complex. As the model becomes more complex and starts fitting the training set too well, it may begin to memorize the training data, and as a result, it will perform poorly on unseen data (test set or real world).\n",
    "\n",
    "It is used to detect when a model starts overfitting, and stop the training process before it becomes too complex. It is usually implemented by monitoring the performance of the model on a validation set during the training process, and stopping the training when the performance on the validation set starts to degrade or stop improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e7409",
   "metadata": {},
   "source": [
    "#### How would explain what a transformer model is to business stakeholders (at a high level)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2718c",
   "metadata": {},
   "source": [
    "A transformer model is a type of neural network architecture that is particularly well-suited for natural language processing tasks. It is designed to handle sequences of variable length and allow the model to attend to different parts of the input sequence, which is important for understanding the relationships between words in a sentence.\n",
    "\n",
    "One of the key advantages of transformer models is that they can be trained on large amounts of data and can be fine-tuned for a wide range of natural language processing tasks, such as language translation, text summarization, and question answering. This makes them a versatile tool for a wide range of business applications such as automated customer service, sentiment analysis, and content generation.\n",
    "\n",
    "In summary, a transformer model is a sophisticated machine learning model that excels in handling and understanding natural language. Its ability to process sequences and attend to different parts of it allows it to be fine-tuned for various NLP tasks and can be beneficial to a business's automated customer service, sentiment analysis, content generation and other similar applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
