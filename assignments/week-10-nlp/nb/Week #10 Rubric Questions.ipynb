{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c092cd",
   "metadata": {},
   "source": [
    "# Week 10 Rubric Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905eb6e",
   "metadata": {},
   "source": [
    "### How does the Naive Bayes Classifier work? What is Posterior Probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69c196",
   "metadata": {},
   "source": [
    "Naive Bayes classifier is a probabilistic machine learning algorithm based on Bayes' theorem. The Naive Bayes classifier makes the \"naive\" assumption that all the features in the input are independent of each other. This means that the likelihood of the input given the class can be calculated by multiplying the likelihoods of each feature given the class. The algorithm starts by calculating the prior probability of each class in the dataset. Then, for a given input, it calculates the likelihood of each feature given each class. It then multiplies these likelihoods together and multiplies the result by the prior probability of each class, to get the probability of the class given the input. The class with the highest probability is chosen as the prediction for the input.\n",
    "\n",
    "Posterior probability is the updated probability of an event or hypothesis after incorporating new information. It is calculated by multiplying the prior probability of the event with the likelihood of the new data given the event, and then normalizing it with the total probability of the new data, regardless of the event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d0e14",
   "metadata": {},
   "source": [
    "### What is the difference between stemming and lemmatization in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a49ca3",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are both techniques used to preprocess text data and make it easier to analyze and understand. \n",
    "\n",
    "- **Stemming**: Stemming is the process of reducing a word to its root or base form by removing the suffix or inflection. It uses heuristics and rule-based approaches to chop off the end of a word. Stemming is fast, but it can be imprecise and may result in non-words.\n",
    "\n",
    "- **Lemmatization**: Lemmatization is the process of reducing a word to its root or base form, but it uses a dictionary or a morphological analysis to look up the correct base form of a word. This results in a valid word that can be found in a dictionary. Lemmatization is more accurate but it's computationally expensive.\n",
    "\n",
    "In summary, stemming is a crude heuristic method that simply chops off the end of a word, while lemmatization is a more refined method that uses knowledge of the language to determine the correct base form of a word. Lemmatization is slower, but it results in valid words that can be found in a dictionary, while stemming can produce non-words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293fdf3",
   "metadata": {},
   "source": [
    "### What is Word2Vec and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7638c6d",
   "metadata": {},
   "source": [
    "Word2Vec is based on the idea that words that occur in similar contexts tend to have similar meanings. The technique uses a neural network to learn these vector representations, which are then used as input for other models such as neural networks for text classification or machine translation.\n",
    "\n",
    "It uses a neural network with a single hidden layer, the input to the network is one-hot encoded representation of the word, and the output is the probability of the context words. The training objective is to maximize the probability of the context words given the target word. Once the model is trained, the weights of the hidden layer are used as the vector representations of words, these vectors are also known as word embeddings. These embeddings can be used as input to other models such as neural networks for text classification or machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf2a44",
   "metadata": {},
   "source": [
    "### When to use GRU over LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbc155",
   "metadata": {},
   "source": [
    "Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) are both types of recurrent neural network that are used to process sequential data such as time series or natural language. Both have a similar structure, but they differ in the way they handle and update information over time. \n",
    "\n",
    "Differences in architecture and considerations for use:\n",
    "- **GRUs** have two gates: an update gate and a reset gate. The update gate controls how much of the previous state is passed to the current state, while the reset gate controls how much of the previous state is forgotten. This allows the model to selectively choose which information to keep and which to discard, making it more efficient than LSTMs.\n",
    "    - GRUs are a more recent development than LSTMs and are often considered a simplified version of LSTMs. They have fewer parameters and are computationally less expensive than LSTMs.\n",
    "    - In general, if computational resources are limited or if you have a large dataset and you need a faster training process, GRU is a better choice.\n",
    "    \n",
    "    \n",
    "- **LSTMs** have three gates: an input gate, an output gate, and a forget gate. The input gate controls how much new information is added to the current state, the output gate controls how much of the previous state is passed to the current state and the forget gate controls how much of the previous state is forgotten.\n",
    "    - Better suited for tasks that require more complex memory management.\n",
    "    - They are able to selectively preserve or discard information, which allows them to handle more complex dependencies and longer time lags. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
