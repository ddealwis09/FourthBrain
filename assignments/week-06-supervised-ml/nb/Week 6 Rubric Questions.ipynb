{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a694640e",
   "metadata": {},
   "source": [
    "# Week 6 Rubric Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a63f6",
   "metadata": {},
   "source": [
    "**Is SVM (Support Vector Machine) a supervised or unsupervised learning algorithm?\n",
    "Why is SVM such a powerful classification method?\n",
    "What are 3 disadvantages of SVMs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d942c",
   "metadata": {},
   "source": [
    "SVM is a supervised machine learning algorithm\n",
    "\n",
    "SVMs are power for classification tasks because they can classify both linear and non-linearly separable data points in n-dimensional space. They can handle non-linear classification by use of kernels. SVMs are also computationally efficient. \n",
    "\n",
    "Three drawbacks of SVMs:\n",
    "- for very large data sets, they are not well suited. Neural networks would be better here. \n",
    "- there is no probability assigned to the classifications it makes\n",
    "- doesn't perform well for overlapping target classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc11468",
   "metadata": {},
   "source": [
    "**What is the time complexity of SVM? \n",
    "What is it for Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470110e",
   "metadata": {},
   "source": [
    "let d = number of dimensions; n = number of data points; and nsv = number of support vectors where d < n:\n",
    "\n",
    "the time complexity of linear SVM is then O(d) and for non-linear (kernel) SVM between O(nsv x d)\n",
    "\n",
    "for logistic regression it is O(n x d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df7ddd",
   "metadata": {},
   "source": [
    "**Explain feature importance for the Random Forest algorithm?\n",
    "When examining feature importance, what is Gini impurity or information gain?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86485707",
   "metadata": {},
   "source": [
    "In the random forest algorithm, feature importance is computed as the decrease in node impurity weighted by the probability of reaching a given node. The probability is weight of samples that reach a given node (divided by the total number of samples). The higher this number, the more important the feature. \n",
    "\n",
    "Gini impurity is used to determine how the features of a dataset should split nodes to form the tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b790b0",
   "metadata": {},
   "source": [
    "**SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model, what is it and how does it work?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f9428",
   "metadata": {},
   "source": [
    "The Shapley value is to measure the contributions to the final outcome from each player separately among the coalition, while preserving the sum of contributions being equal to the final outcome. \n",
    "\n",
    "The goal of SHAP is to explain a prediction by calculating the contribution of each feature to the prediction. SHAP attempts to determine how to distribute the predictions among the features. \n",
    "\n",
    "Essentially, the Shapley value is the average expected marginal contribution of one player after all possible combinations have been considered. Shapley value helps to determine a payoff for all of the features when each feature might have contributed more or less than the others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
