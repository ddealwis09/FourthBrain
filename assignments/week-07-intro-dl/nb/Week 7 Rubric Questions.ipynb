{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dffc3390",
   "metadata": {},
   "source": [
    "# Week 7 Rubric Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b373f3",
   "metadata": {},
   "source": [
    "### What is Normalization and how does Normalization make training a model more stable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f489b",
   "metadata": {},
   "source": [
    "Normalization is the process whereby we transform the original features in our data set that have varying magnitudes and ranges to a common scale without distorting the differences in the ranges of values, thereby keeping the information intact about the relationship with our target feature. \n",
    "\n",
    "Normalization improves performance and training stability of the model. It does this because different scales of inputs cause different weights updates and optimizers steps towards the minimum. It also makes the shape of the loss function disproportional. In that case, we need to use a lower learning rate to not overshoot, which means a slower learning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c730946",
   "metadata": {},
   "source": [
    "### What are loss and optimizer functions and how do they work? What is Gradient Descent and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e527e",
   "metadata": {},
   "source": [
    "The loss function or cost function is the function that is used to evaluate the performance of a machine learning model, i.e. how well it makes predictions. Different models typically specify different loss functions that are best suited for the type of problem at hand e.g. mean-squared error for linear regression.\n",
    "\n",
    "The optimizer is what is used to minimize the loss function. After each epoch the weights and biases are calculated and the optimizer determines how that happens. \n",
    "\n",
    "Gradient descent is the algorithm used in machine learning to iteratively find the minima or maxima of a cost function. It requires the function be differentiable and convex to the origin. It works by first initalizing a starting point (a guess) as to where the minima or maxima might be. It then iteratively computes the derivative at that point and determines if it should move up or down to find the optima. The learning rate of the algorithm can speed up gradient descent's convergence by randomly moving it's position around its current position by larger steps than if gradient descent had to compute derivatives at every single point of the loss function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79645b",
   "metadata": {},
   "source": [
    "### What is an activation function? What are the outputs of the following activation functions: ReLU, Softmax Tanh, Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a515325",
   "metadata": {},
   "source": [
    "Activation functions are functions that are applied to inputs and take on a task of transforming that input in some way. In a neural network, they decide on whether a given neuron should be \"activated\" or not. Thereby determining if the outputs of that neuron are important or not in making predictions. \n",
    "\n",
    "Outputs for the following activation functions:\n",
    "- ReLU: outputs the input directly if positive, otherwise zero\n",
    "- Softmax (n dimensional output):outputs the probability distribution for the liklihood of a given input belonging to a class \n",
    "- Tanh (binary output): output ranges from -1 to 1 with negative inputs have a strong mapping to -1 and positive inouts will have a strong mapping to +1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37089a40",
   "metadata": {},
   "source": [
    "### What is the TPOT algorithm and how does it work? What does TPOT stand for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bfebfe",
   "metadata": {},
   "source": [
    "TPOT stands for \"tree-based pipleline optimization tool\".\n",
    "\n",
    "TPOT is an automated machine learning tool built on top of scikit learn that optimizes machine learning pipelines using genetic programming. After it runs on your dataset, it provides you with the code for the optimal pipeline so that you can make modifications. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
